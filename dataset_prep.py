# -*- coding: utf-8 -*-
"""Dataset_Prep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q4KdAHCjFtMAL1Z2HSsz9qhuAoUGZrXx
"""

# ğŸ› ï¸ Step 1: Install dependencies
!pip install datasets tqdm

# ğŸ“¥ Step 2: Load dataset
from datasets import load_dataset
import os
import requests
from tqdm import tqdm

# ğŸ“‚ Create output folder
os.makedirs("reports", exist_ok=True)

# ğŸ§± Load the ESG report dataset
dataset = load_dataset("DataNeed/company-reports")
data = dataset[list(dataset.keys())[0]]
print(f"ğŸ“Š Total reports in dataset: {len(data)}")

# ğŸ”— Extract URLs and filename map
def get_filename_from_url(url):
    return os.path.basename(url.split("?")[0])

urls = [item['url'] for item in data]
url_map = {get_filename_from_url(u): u for u in urls}

# ğŸ“œ Load download logs
download_log_path = "downloaded.log"
failed_log_path = "failed.log"

downloaded = set()
failed = set()

if os.path.exists(download_log_path):
    with open(download_log_path) as f:
        downloaded = set(line.strip() for line in f if line.strip())

if os.path.exists(failed_log_path):
    with open(failed_log_path) as f:
        failed = set(line.strip().split("\t")[0] for line in f if line.strip())

# ğŸ§® Determine remaining files to download
remaining = {filename: url for filename, url in url_map.items() if filename not in downloaded and filename not in failed}
print(f"ğŸ”„ Remaining files to download: {len(remaining)}")

# ğŸ” Download loop with accurate progress bar
headers = {"User-Agent": "Mozilla/5.0"}
success_count, fail_count = 0, 0

with open(download_log_path, "a") as dl_log, open(failed_log_path, "a") as fail_log:
    for filename, url in tqdm(remaining.items(), total=len(remaining), desc="Downloading reports"):
        dest_path = os.path.join("reports", filename)

        try:
            response = requests.get(url, headers=headers, timeout=15)
            if response.status_code == 200 and response.content:
                with open(dest_path, "wb") as f:
                    f.write(response.content)
                dl_log.write(filename + "\n")
                dl_log.flush()
                success_count += 1
            else:
                fail_log.write(f"{filename}\t{url}\t{response.status_code}\n")
                fail_log.flush()
                fail_count += 1
        except Exception as e:
            fail_log.write(f"{filename}\t{url}\tERROR: {str(e)}\n")
            fail_log.flush()
            fail_count += 1

print(f"âœ… Done! {success_count} downloaded, {fail_count} failed.")
print(f"ğŸ“¦ {len(downloaded)} already downloaded, {len(failed)} previously failed.")

# Step 1: Zip the folder
!zip -r reports.zip reports

# Step 2: Create download link
from google.colab import files
files.download("reports.zip")

# ğŸ› ï¸ Step 1: Install dependencies
!pip install pymupdf

import os
import fitz  # PyMuPDF
import json
import glob
import re
from tqdm import tqdm

# ğŸ“ Step 2: Create output folder
os.makedirs("json_reports", exist_ok=True)

# ğŸ§  Step 3: Helper functions
def extract_meta_from_filename(filename):
    name = os.path.basename(filename).replace(".pdf", "")
    match = re.match(r"(.+?)_(\d{4})", name)
    if match:
        return match.group(1).title(), match.group(2)
    return "Unknown", "Unknown"

def pdf_to_json(pdf_path):
    doc = fitz.open(pdf_path)
    pages = [{"page_number": i+1, "text": page.get_text()} for i, page in enumerate(doc)]
    full_text = "\n".join(p["text"] for p in pages)
    company, year = extract_meta_from_filename(pdf_path)

    return {
        "company": company,
        "year": year,
        "filename": os.path.basename(pdf_path),
        "text": full_text,
        "pages": pages
    }

# ğŸ“š Step 4: Convert PDFs and aggregate
pdf_files = glob.glob("reports/*.pdf")
all_data = []

for pdf_path in tqdm(pdf_files, desc="Converting PDFs"):
    try:
        json_data = pdf_to_json(pdf_path)
        all_data.append(json_data)

        # Save individual JSONs (optional)
        output_path = os.path.join("json_reports", os.path.basename(pdf_path).replace(".pdf", ".json"))
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ Skipping {os.path.basename(pdf_path)}: {e}")

# ğŸ’¾ Step 5: Save all as a single concatenated JSON array
with open("all_reports.json", "w", encoding="utf-8") as f:
    json.dump(all_data, f, ensure_ascii=False, indent=2)

# ğŸ“¦ Step 6: Zip individual JSONs (optional)
!zip -r json_reports.zip json_reports

print("âœ… Done! You can now download 'all_reports.json' or 'json_reports.zip'")

import os
import json
from tqdm import tqdm

json_folder = "json_reports"  # or path to your output
broken = []
empty = []
too_short = []

for file in tqdm(os.listdir(json_folder)):
    if not file.endswith(".json"):
        continue
    try:
        with open(os.path.join(json_folder, file), encoding="utf-8") as f:
            data = json.load(f)
            text = data.get("text", "")
            if not text.strip():
                empty.append(file)
            elif len(text.strip()) < 500:
                too_short.append(file)
    except Exception as e:
        broken.append((file, str(e)))

print(f"âœ… Total JSON files: {len(os.listdir(json_folder))}")
print(f"ğŸ“­ Empty files: {len(empty)}")
print(f"ğŸ“‰ Too short (<500 chars): {len(too_short)}")
print(f"âŒ Failed to parse: {len(broken)}")